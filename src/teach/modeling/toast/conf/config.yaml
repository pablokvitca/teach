model_type: task_from_text_single_game
known_model_data_types:
  naive
  gru_text
  gru_text_subgoal
  task_from_text_single
  task_from_text_single_game
skip_model_load: True
parent_dir_path: MUST BE SET IN CLI ARGS PLEASE
data_folder_path: ${parent_dir_path}teach-dataset
model_checkpoints_pre_path: model_checkpoints_out
model_load_name: epoch=0-step=44.ckpt
naive:
  wv2_path:  ${parent_dir_path}GoogleNews-vectors-negative300.bin.gz
  x_text_pad_length: 100
  x_prev_action_pad_length: 100
  auto_lr_find: True
gru_text:
  input_lang_path: ${parent_dir_path}teach_input_lang.pickle
  output_lang_path: ${parent_dir_path}teach_output_lang.pickle
  save_input_lang_path: ${parent_dir_path}new_teach_input_lang.pickle
  save_output_lang_path: ${parent_dir_path}new_teach_output_lang.pickle
  encoder_hidden_size: 256
  decoder_hidden_size: 256
  teacher_forcing: True
  decoder_dropout_p: 0.5
  auto_lr_find: False
  max_length: 1000
  max_output_length: 1000
  output_length_delta: 10
  use_single_optimizer: False
gru_text_subgoal:
  input_lang_path: None
  output_lang_path: ${parent_dir_path}teach_subgoal_output_lang.pickle
  save_input_lang_path: ${parent_dir_path}new_teach_subgoal_input_lang.pickle
  save_output_lang_path: ${parent_dir_path}new_teach_subgoal_output_lang.pickle
  encoder_hidden_size: 256
  decoder_hidden_size: 256
  teacher_forcing: True
  decoder_dropout_p: 0.5
  auto_lr_find: False
  max_length: 1000
  max_output_length: 1000
  output_length_delta: 10
  use_subgoal_history: True
  use_subgoal_future: True
  use_commander_language: True
  use_follower_language: True
  use_single_optimizer: False
task_from_text_single:
  pretrained_model_name: distilbert-base-uncased
  use_commander_language: True
  use_follower_language: True
  auto_lr_find: False
  learning_rate: 2e-5
  adam_epsilon: 1e-8
  warmup_steps: 0
  weight_decay: 0.0
  insert_pad_token: False
  freeze_encoder: True
task_from_text_single_game:
  pretrained_model_name: distilbert-base-uncased
  use_commander_language: True
  use_follower_language: True
  auto_lr_find: False
  learning_rate: 2e-5
  adam_epsilon: 1e-8
  warmup_steps: 0
  weight_decay: 0.0
  insert_pad_token: False
  freeze_encoder: True
task_from_text_multi:
  pretrained_model_name: distilbert-base-uncased
  use_commander_language: True
  use_follower_language: True
  auto_lr_find: False
  learning_rate: 2e-5
  adam_epsilon: 1e-8
  warmup_steps: 0
  weight_decay: 0.0
  insert_pad_token: False
trainer:
  checkpoints_save_top_k: 3
  acc_device: gpu
  devices: '0'
  fallback_devices: 1
  auto_lr_find: False
  auto_batch_find: False
  use_model_lr: True
  track_grad_norm: -1
  gradient_clip_val: 0.0
  gradient_clip_algorithm: value
  max_epochs: 50
  num_sanity_val_steps: 1
  detect_anomaly: False
  fast_dev_run: False
  lr: 2e-5
  check_val_every_n_epoch: 1
  val_check_interval: 1
datamodule:
  num_workers: 1
  batch_size: 16
  validation_batch_size: 1
  use_small_dataset: False
  save_data_preprocessing: False
  fail_if_cannot_load: False
wandb:
  project: toast
  offline: False
  group_name: task_from_text_single
  log_model: False
  entity: 'nlp4robots-toast'